{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQrY/sKhZ7h2AAG1wHTwE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrl7012005/Reinforcement-Learning/blob/main/AI_plays_Snake_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Q Learning"
      ],
      "metadata": {
        "id": "bMcJJWMIR1DJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall reinforcement learning just focuses on maximizing the CUMULATIVE reward"
      ],
      "metadata": {
        "id": "Yrkt4RgaTFQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 actions, turn left, turn right, go straight. So the action vector is 3 dimensional.\n",
        "\n",
        "The reward function we choose will be +10 if you collect the pellets, -10 if you lose.\n",
        "\n",
        "The state will be described as a 11 dimensional vector. 4 to describe which direction we are going (left, right, up, down), 3 to describe which direction the danger is (can be multiple dangers), and 4 to describe which direction the food is."
      ],
      "metadata": {
        "id": "AFJBmui0TFJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q value measures the quality of action. Q(s, a) is the expected cumulative future reward of taking an action a in state s and following the optimal policy.\n",
        "\n",
        "Q-Learning:\n",
        "\n",
        "-Initialize the Q value  \n",
        "-Choose an action either via our Q function or randomly (exploration vs exploitation)  \n",
        "-Perform the action  \n",
        "-Measure the reward  \n",
        "-Update the Q value  \n",
        "\n",
        "Repeat iteratively"
      ],
      "metadata": {
        "id": "jpkO43bxXXyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q value is updated according to the Bellman equation. The Q value is updated as follows\n",
        "\n",
        "Q(s, a)_new = Q(s, a) + alpha * [R(s, a) + gamma * max(Q'(s', a')) - Q(s, a)]\n",
        "\n",
        "-alpha is the learning rate, controlling how much new information overrides the old information    \n",
        "-R(s, a) is the reward for taking that action in that state  \n",
        "-gamma is the discount factor (how much future rewards should be accounted for when compared to immediate rewards)  \n",
        "-Q(s', a')  is the expected future reward given the new state and all possible actions at that new state. This is maximized over all actions.\n",
        "\n",
        "The next action is chosen as the one that maximizes the Q function i.e. the expected cumulative future reward.\n",
        "\n",
        "For regular Q learning, a Q table is used and it would typically be initialized with zeros. The Q table has a value for every state-action pair. Neural networks approximate the Q value in DQN.\n",
        "\n",
        "The Q value is unknown before training, it represents the expected cumulative future reward. For each episode, single run until the agent finishes or fails, we take actions, observe states etc. Everytime we reach a state and do an action, we update the Q value for that state-action. If it was a good action then (good reward) or if in the future the reward was good, then the Q values will have been propagated (because each Q value is updated taking into account future rewards as well), then the Q values for that state-action pair will be high. After many iterations, the Q-value at each time step will contain information long into the future about which actions and states were good, in other words, a policy to follow to maximize cumulative reward.\n",
        "\n",
        "Deep Q learning approximates the Q function through the network's weights and biases. There are 2 components, the Q-Network and the target Q-network. The Q-network is the main network to predict Q values and the target Q-network is a separate network with the same architecture but its weights are periodically updated to match the Q-Network. This stabilizes training by reducing oscillations.\n",
        "\n",
        "Past experiences are stored in a replay memory (buffer). Each experience (frame essentially) is a tuple of state, action taken, reward received, next state and done indicating if the episode has finished. Each experience is stored in a replay buffer. During training, a mini batch is sampled from the buffer. The target Q value for each experience is calculated using the Bellman equation, because this method is the way the Q should be updated. This is also termed Q_new. Now the loss is computed between the target Q-value and the predicted Q-value from the Q-network. This loss is used to update the weights of the Q-network, so the Q-network learns bellman's equation and approximates the way Q functions should evolve. DQN is used for high dimensional state spaces. For the snake\n",
        "\n",
        "We use a decaying epsilon (exploration probability) over time to encourage exploitation.\n"
      ],
      "metadata": {
        "id": "6HAyUg4SXxnx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g9OB_fieyl5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Environment"
      ],
      "metadata": {
        "id": "347JZOgniXoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the environment involves building the snake game. The environment should have a loop where taking in an acton as the input and returning a reward, the current score, the state, and whether or not the game is over."
      ],
      "metadata": {
        "id": "Ot62oGhWieej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pygame is used to build games like this in Python. Enum, use this to define constants."
      ],
      "metadata": {
        "id": "kTGqhjMhjLtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import random\n",
        "from enum import Enum\n",
        "from collections import namedtuple\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG0spSRujPd4",
        "outputId": "0874100e-63fc-44bf-9838-2ebe8f6807de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.6.0 (SDL 2.28.4, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the font size to 25"
      ],
      "metadata": {
        "id": "Ih_hgBjLnY-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pygame.init()\n",
        "font = pygame.font.SysFont('arial', 25)"
      ],
      "metadata": {
        "id": "8ej-HIrCmSSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Direction(Enum):\n",
        "  RIGHT, LEFT, UP, DOWN = 1, 2, 3, 4\n",
        "\n",
        "Point = namedtuple('Point', 'x, y')\n",
        "\n",
        "WHITE = (255, 255, 255)\n",
        "RED = (200, 0, 0)\n",
        "BLUE1 = (0, 0, 255)\n",
        "BLUE2 = (0, 100, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "\n",
        "BLOCK_SIZE = 20\n",
        "SPEED = 40"
      ],
      "metadata": {
        "id": "RbyhjOG0n3qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create the actual snake game. We will define the display with\n",
        "pygame.display.set_mode with caption 'Snake'. Start the time, start at the center, start going right first, initialize variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "tJ8ibrhYpN_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SnakeGameAI():\n",
        "  def __init__(self, w = 640, h = 480):\n",
        "    self.w = w\n",
        "    self.h = h\n",
        "\n",
        "    self.display = pygame.display.set_mode((self.w, self.h))\n",
        "    pygame.display.set_caption('Snake')\n",
        "\n",
        "    #Initialize the time\n",
        "    self.clock = pygame.time.Clock()\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    #Initialize the direction moving\n",
        "    self.direction = Direction.RIGHT\n",
        "\n",
        "    #Initialize a head at the center\n",
        "    self.head = Point(self.w/2, self.h/2)\n",
        "\n",
        "    #Initialize the snake with 3 blocks\n",
        "    self.snake = [self.head, Point(self.head.x - BLOCK_SIZE, self.head.y),\n",
        "                  Point(self.head.x - 2 * BLOCK_SIZE, self.head.y)]\n",
        "\n",
        "    self.score = 0\n",
        "    self.food = None\n",
        "    self._place_food()\n",
        "\n",
        "    self.frame_iteration = 0\n",
        "\n",
        "  #randomly place a food at a point, if inside the snake, free point (grid)\n",
        "  def _place_food(self):\n",
        "    x = random.randint(0, (self.w - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    y = random.randint(0, (self.h - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    self.food = Point(x, y)\n",
        "\n",
        "    if self.food in self.snake:\n",
        "      self._place_food()\n",
        "\n",
        "\n",
        "  def play_step(self, action):\n",
        "\n",
        "    self.frame_iteration += 1\n",
        "    for event in pygame.event.get():\n",
        "\n",
        "      if event.type == pygame.QUIT:\n",
        "        pygame.quit()\n",
        "        quit()\n",
        "\n",
        "    #Move the head\n",
        "    self._move(action)\n",
        "\n",
        "    #Grow the snake\n",
        "    self.snake.insert(0, self.head)\n",
        "\n",
        "    #Check if game is over\n",
        "    reward = 0\n",
        "    game_over = False\n",
        "    if self._is_collision() or self.frame_iteration > 100 * len(self.snake):\n",
        "      game_over = True\n",
        "      reward = -10\n",
        "      return game_over, self.score\n",
        "\n",
        "    #Place new food or dont grow\n",
        "\n",
        "    if self.head == self.food:\n",
        "      self.score += 1\n",
        "      reward = 10\n",
        "      self._place_food()\n",
        "    else:\n",
        "      self.snake.pop()\n",
        "\n",
        "    #update the ui and clock\n",
        "    self._update_ui()\n",
        "\n",
        "    #the timestep\n",
        "    self.clock.tick(SPEED)\n",
        "\n",
        "    return reward, game_over, self.score\n",
        "\n",
        "  def _is_collision(self, pt=None):\n",
        "\n",
        "    if pt is None:\n",
        "      pt = self.head\n",
        "\n",
        "    if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
        "      return True\n",
        "\n",
        "    if self.head in self.snake[1:]:\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _update_ui(self):\n",
        "\n",
        "    self.display.fill(BLACK)\n",
        "\n",
        "    #Draw the blue snake\n",
        "    for pt in self.snake:\n",
        "      pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "      pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x + 4, pt.y + 4, 12, 12))\n",
        "\n",
        "    #Draw the red food\n",
        "    pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "\n",
        "    #Update the font\n",
        "    text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
        "    self.display.blit(text, [0, 0])\n",
        "    pygame.display.flip()\n",
        "\n",
        "  def _move(self, action):\n",
        "\n",
        "    clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
        "    idx = clock_wise.index(self.direction)\n",
        "\n",
        "    if np.array_equal(action, [1, 0, 0]):\n",
        "      new_dir = clock_wise[idx]\n",
        "    elif np.array_equal(action, [0, 1, 0]):\n",
        "      next_idx = (idx + 1) % 4\n",
        "      new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
        "    else: # [0, 0, 1]\n",
        "      next_idx = (idx - 1) % 4\n",
        "      new_dir = clock_wise[next_idx]\n",
        "\n",
        "    self.direction = new_dir\n",
        "\n",
        "    x = self.head.x\n",
        "    y = self.head.y\n",
        "\n",
        "    if self.direction == Direction.RIGHT:\n",
        "        x += BLOCK_SIZE\n",
        "    elif self.direction == Direction.LEFT:\n",
        "        x -= BLOCK_SIZE\n",
        "    elif self.direction == Direction.DOWN:\n",
        "        y += BLOCK_SIZE\n",
        "    elif self.direction == Direction.UP:\n",
        "        y -= BLOCK_SIZE\n",
        "\n",
        "    self.head = Point(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJjsPhqpPQs",
        "outputId": "ad5ca3fe-97fc-4bf4-a554-e341adafc6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Score 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating the agent"
      ],
      "metadata": {
        "id": "ZiSTw1MrN0g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the agent class we want to dictate how the agent interacts with the environment, essentially the processing and bulk of the code. We want to write what we do with the actions, how do we get the next state etc.\n",
        "\n",
        "We need a function to get the state of the game, a function to remember, functions to train the Q-network and the target Q-network"
      ],
      "metadata": {
        "id": "qgsz4w8WOfXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque #queue data structure\n",
        "\n",
        "\n",
        "MAX_MEMORY = 100000\n",
        "BATCH_SIZE = 1000\n",
        "LR = 0.001\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self):\n",
        "\n",
        "    self.ngames = 0\n",
        "    self.epsilon = 0 #randomness\n",
        "    self.gamma = 0.9\n",
        "    self.memory = deque(maxlen=MAX_MEMORY)\n",
        "    self.model = Linear_QNet(11, 256, 3) #We'll create the Q nets below\n",
        "    self.target_model = QTrainer(self.model, lr=LR, gamma=self.gamma))\n",
        "\n",
        "  def get_state(self, game):\n",
        "    #game is an instance of the SnakeGameAI\n",
        "    head = game.snake[0]\n",
        "\n",
        "    point_l = Point(head.x - 20, head.y)\n",
        "    point_r = Point(head.x + 20, head.y)\n",
        "    point_u = Point(head.x, head.y - 20)\n",
        "    point_d = Point(head.x, head.y + 20)\n",
        "\n",
        "    #one hot encode the direction of movement\n",
        "    dir_l = game.direction == Direction.LEFT\n",
        "    dir_r = game.direction == Direction.RIGHT\n",
        "    dir_u = game.direction == Direction.UP\n",
        "    dir_d = game.direction == Direction.DOWN\n",
        "\n",
        "    state = [\n",
        "        #Danger straight\n",
        "        # Danger straight\n",
        "            (dir_r and game.is_collision(point_r)) or\n",
        "            (dir_l and game.is_collision(point_l)) or\n",
        "            (dir_u and game.is_collision(point_u)) or\n",
        "            (dir_d and game.is_collision(point_d)),\n",
        "\n",
        "            # Danger right\n",
        "            (dir_u and game.is_collision(point_r)) or\n",
        "            (dir_d and game.is_collision(point_l)) or\n",
        "            (dir_l and game.is_collision(point_u)) or\n",
        "            (dir_r and game.is_collision(point_d)),\n",
        "\n",
        "            # Danger left\n",
        "            (dir_d and game.is_collision(point_r)) or\n",
        "            (dir_u and game.is_collision(point_l)) or\n",
        "            (dir_r and game.is_collision(point_u)) or\n",
        "            (dir_l and game.is_collision(point_d)),\n",
        "\n",
        "            # Move direction\n",
        "            dir_l,\n",
        "            dir_r,\n",
        "            dir_u,\n",
        "            dir_d,\n",
        "\n",
        "            # Food location\n",
        "            game.food.x < game.head.x,  # food left\n",
        "            game.food.x > game.head.x,  # food right\n",
        "            game.food.y < game.head.y,  # food up\n",
        "            game.food.y > game.head.y  # food down\n",
        "\n",
        "    ]\n",
        "\n",
        "    return np.array(state, dtype=int)\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    #Store state, action, reward, next_state, done in a queue for training later\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def train_long_memory(self):\n",
        "    if len(self.memory) > BATCH_SIZE:\n",
        "      #Train a batch at a time on the target Q network\n",
        "      mini_sample = random.sample(self.memory, BATCH_SIZE)\n",
        "    else:\n",
        "      mini_sample = self.memory\n",
        "\n",
        "    #zip * separates out the entries by index of each entry\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
        "    self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "  def train_short_memory(self, state, action, reward, next_state, done):\n",
        "    self.trainer.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "  def get_action(self, state):\n",
        "    self.epsilon = 80 - self.ngames #slowly decay epsilon\n",
        "    final_move = [0, 0, 0]\n",
        "    if random.randint(0, 200) < self.epsilon:\n",
        "      #explore in random direction\n",
        "      move = random.randint(0, 2)\n",
        "      final_move[move] = 1\n",
        "    else:\n",
        "      state0 = torch.tensor(state, dtype=torch.float)\n",
        "      prediction = self.model(state0)\n",
        "      move = torch.argmax(prediction).item()\n",
        "      final_move[move] = 1\n",
        "\n",
        "    return final_move\n",
        "\n",
        "def train():\n",
        "\n",
        "  plot_scores = []\n",
        "  plot_mean_scores = []\n",
        "  total_score = 0\n",
        "  record = 0\n",
        "  agent = Agent()\n",
        "  game = SnakeGameAI()\n",
        "\n",
        "  while True:\n",
        "\n",
        "    state_old = agent.get_state(game)\n",
        "    final_move = agent.get_action(state_old)\n",
        "\n",
        "    #interact with environment\n",
        "    reward, done, score = game.play_step(final_move)\n",
        "\n",
        "    state_new = agent.get_state(game)\n",
        "\n",
        "    #Update the Q network\n",
        "    agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    agent.remember(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    if done:\n",
        "      #if finish an episode\n",
        "      game.reset()\n",
        "      agent.n_games += 1\n",
        "      agent.train_long_memory()\n",
        "\n",
        "      if score > record:\n",
        "        record = score\n",
        "        agent.model.save()\n",
        "\n",
        "      print('Game', agent.ngames, 'Score', score, 'Record:', record)\n",
        "\n",
        "      plot_scores.append(score)\n",
        "      total_score += score\n",
        "      mean_score = total_score / agent.n_games\n",
        "      plot_mean_scores.append(mean_score)\n",
        "\n",
        "      plot(plot_scores, plot_mean_scores)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train()"
      ],
      "metadata": {
        "id": "qrmed2y8OMJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "TgoYsbF-VZI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets build the networks and the code to train the Q-networks. The Q-net contains only 1 hidden layer."
      ],
      "metadata": {
        "id": "pbKz9xzMSiZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "class Linear_QNet(nn.Module):\n",
        "  #target Q network\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear2(F.relu(self.linear1(x)))\n",
        "    return x\n",
        "\n",
        "  def save(self, filename='model.pth'):\n",
        "    model_path = './model'\n",
        "    if not os.path.exists(model_path):\n",
        "      os.makedirs(model_path)\n",
        "\n",
        "    filename = os.path.join(model_path, filename)\n",
        "\n",
        "    torch.save(self.state_dict(), filename)\n",
        "\n",
        "class QTrainer():\n",
        "  def __init__(self, model, lr, gamma):\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.model = model\n",
        "    self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def train_step(self, state, action, reward, next_state, done):\n",
        "\n",
        "    state = torch.tensor(state, dtype=torch.float)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "    action = torch.tensor(action, dtype=torch.long)\n",
        "    reward = torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    if len(state.shape) == 1:\n",
        "      state = torch.unsqueeze(state, 0)\n",
        "      next_state = torch.unsqueeze(next_state, 0)\n",
        "      action = torch.unsqueeze(action, 0)\n",
        "      reward = torch.unsqueeze(reward, 0)\n",
        "      done = (done, )\n",
        "\n",
        "    pred = self.model(state)\n",
        "\n",
        "    target = pred.clone()\n",
        "\n",
        "    for idx in range(len(done)):\n",
        "\n",
        "      Q_new = reward[idx]\n",
        "      if not done[idx]:\n",
        "        Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
        "\n",
        "      target[idx][torch.argmax(action).item()] = Q_new\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss = self.criterion(target, pred)\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "id": "5C4-AJqtSl9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need one more function to plot the learning curve"
      ],
      "metadata": {
        "id": "MabnuE8kVODK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def plot(scores, mean_scores):\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())\n",
        "    plt.clf()\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Number of Games')\n",
        "    plt.ylabel('Score')\n",
        "    plt.plot(scores)\n",
        "    plt.plot(mean_scores)\n",
        "    plt.ylim(ymin=0)\n",
        "    plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n",
        "    plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n",
        "    plt.show(block=False)\n",
        "    plt.pause(.1)"
      ],
      "metadata": {
        "id": "XV-YmlZ2VUR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok great everything is done, just put it all into scripts and download"
      ],
      "metadata": {
        "id": "owX0DBjUVJbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scripts"
      ],
      "metadata": {
        "id": "dYx-PnLHVc-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper.py\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "def plot(scores, mean_scores):\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(plt.gcf())\n",
        "    plt.clf()\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Number of Games')\n",
        "    plt.ylabel('Score')\n",
        "    plt.plot(scores)\n",
        "    plt.plot(mean_scores)\n",
        "    plt.ylim(ymin=0)\n",
        "    plt.text(len(scores)-1, scores[-1], str(scores[-1]))\n",
        "    plt.text(len(mean_scores)-1, mean_scores[-1], str(mean_scores[-1]))\n",
        "    plt.show(block=False)\n",
        "    plt.pause(.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvsjiZFRVeLx",
        "outputId": "703b70d9-8de4-4bde-d311-03b66d35be22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "class Linear_QNet(nn.Module):\n",
        "  #target Q network\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "    self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear2(F.relu(self.linear1(x)))\n",
        "    return x\n",
        "\n",
        "  def save(self, filename='model.pth'):\n",
        "    model_path = './model'\n",
        "    if not os.path.exists(model_path):\n",
        "      os.makedirs(model_path)\n",
        "\n",
        "    filename = os.path.join(model_path, filename)\n",
        "\n",
        "    torch.save(self.state_dict(), filename)\n",
        "\n",
        "class QTrainer():\n",
        "  def __init__(self, model, lr, gamma):\n",
        "    self.lr = lr\n",
        "    self.gamma = gamma\n",
        "    self.model = model\n",
        "    self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def train_step(self, state, action, reward, next_state, done):\n",
        "\n",
        "    state = torch.tensor(state, dtype=torch.float)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float)\n",
        "    action = torch.tensor(action, dtype=torch.long)\n",
        "    reward = torch.tensor(reward, dtype=torch.float)\n",
        "\n",
        "    if len(state.shape) == 1:\n",
        "      state = torch.unsqueeze(state, 0)\n",
        "      next_state = torch.unsqueeze(next_state, 0)\n",
        "      action = torch.unsqueeze(action, 0)\n",
        "      reward = torch.unsqueeze(reward, 0)\n",
        "      done = (done, )\n",
        "\n",
        "    pred = self.model(state)\n",
        "\n",
        "    target = pred.clone()\n",
        "\n",
        "    for idx in range(len(done)):\n",
        "\n",
        "      Q_new = reward[idx]\n",
        "      if not done[idx]:\n",
        "        Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
        "\n",
        "      target[idx][torch.argmax(action).item()] = Q_new\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    loss = self.criterion(target, pred)\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl0de59CVkTS",
        "outputId": "09bc27a8-33b6-46ba-8049-f489cdc8fc7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile agent.py\n",
        "\n",
        "#our main function\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque #queue data structure\n",
        "from game import SnakeGameAI, Direction, Point\n",
        "from model import Linear_QNet, QTrainer\n",
        "from helper import plot\n",
        "\n",
        "\n",
        "MAX_MEMORY = 100000\n",
        "BATCH_SIZE = 1000\n",
        "LR = 0.001\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self):\n",
        "\n",
        "    self.ngames = 0\n",
        "    self.epsilon = 0 #randomness\n",
        "    self.gamma = 0.9\n",
        "    self.memory = deque(maxlen=MAX_MEMORY)\n",
        "    self.model = Linear_QNet(11, 256, 3) #We'll create the Q nets below\n",
        "    self.target_model = QTrainer(self.model, lr=LR, gamma=self.gamma))\n",
        "\n",
        "  def get_state(self, game):\n",
        "    #game is an instance of the SnakeGameAI\n",
        "    head = game.snake[0]\n",
        "\n",
        "    point_l = Point(head.x - 20, head.y)\n",
        "    point_r = Point(head.x + 20, head.y)\n",
        "    point_u = Point(head.x, head.y - 20)\n",
        "    point_d = Point(head.x, head.y + 20)\n",
        "\n",
        "    #one hot encode the direction of movement\n",
        "    dir_l = game.direction == Direction.LEFT\n",
        "    dir_r = game.direction == Direction.RIGHT\n",
        "    dir_u = game.direction == Direction.UP\n",
        "    dir_d = game.direction == Direction.DOWN\n",
        "\n",
        "    state = [\n",
        "        #Danger straight\n",
        "        # Danger straight\n",
        "            (dir_r and game.is_collision(point_r)) or\n",
        "            (dir_l and game.is_collision(point_l)) or\n",
        "            (dir_u and game.is_collision(point_u)) or\n",
        "            (dir_d and game.is_collision(point_d)),\n",
        "\n",
        "            # Danger right\n",
        "            (dir_u and game.is_collision(point_r)) or\n",
        "            (dir_d and game.is_collision(point_l)) or\n",
        "            (dir_l and game.is_collision(point_u)) or\n",
        "            (dir_r and game.is_collision(point_d)),\n",
        "\n",
        "            # Danger left\n",
        "            (dir_d and game.is_collision(point_r)) or\n",
        "            (dir_u and game.is_collision(point_l)) or\n",
        "            (dir_r and game.is_collision(point_u)) or\n",
        "            (dir_l and game.is_collision(point_d)),\n",
        "\n",
        "            # Move direction\n",
        "            dir_l,\n",
        "            dir_r,\n",
        "            dir_u,\n",
        "            dir_d,\n",
        "\n",
        "            # Food location\n",
        "            game.food.x < game.head.x,  # food left\n",
        "            game.food.x > game.head.x,  # food right\n",
        "            game.food.y < game.head.y,  # food up\n",
        "            game.food.y > game.head.y  # food down\n",
        "\n",
        "    ]\n",
        "\n",
        "    return np.array(state, dtype=int)\n",
        "\n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "    #Store state, action, reward, next_state, done in a queue for training later\n",
        "    self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def train_long_memory(self):\n",
        "    if len(self.memory) > BATCH_SIZE:\n",
        "      #Train a batch at a time on the target Q network\n",
        "      mini_sample = random.sample(self.memory, BATCH_SIZE)\n",
        "    else:\n",
        "      mini_sample = self.memory\n",
        "\n",
        "    #zip * separates out the entries by index of each entry\n",
        "    states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
        "    self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "  def train_short_memory(self, state, action, reward, next_state, done):\n",
        "    self.trainer.train_step(state, action, reward, next_state, done)\n",
        "\n",
        "  def get_action(self, state):\n",
        "    self.epsilon = 80 - self.ngames #slowly decay epsilon\n",
        "    final_move = [0, 0, 0]\n",
        "    if random.randint(0, 200) < self.epsilon:\n",
        "      #explore in random direction\n",
        "      move = random.randint(0, 2)\n",
        "      final_move[move] = 1\n",
        "    else:\n",
        "      state0 = torch.tensor(state, dtype=torch.float)\n",
        "      prediction = self.model(state0)\n",
        "      move = torch.argmax(prediction).item()\n",
        "      final_move[move] = 1\n",
        "\n",
        "    return final_move\n",
        "\n",
        "def train():\n",
        "\n",
        "  plot_scores = []\n",
        "  plot_mean_scores = []\n",
        "  total_score = 0\n",
        "  record = 0\n",
        "  agent = Agent()\n",
        "  game = SnakeGameAI()\n",
        "\n",
        "  while True:\n",
        "\n",
        "    state_old = agent.get_state(game)\n",
        "    final_move = agent.get_action(state_old)\n",
        "\n",
        "    #interact with environment\n",
        "    reward, done, score = game.play_step(final_move)\n",
        "\n",
        "    state_new = agent.get_state(game)\n",
        "\n",
        "    #Update the Q network\n",
        "    agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    agent.remember(state_old, final_move, reward, state_new, done)\n",
        "\n",
        "    if done:\n",
        "      #if finish an episode\n",
        "      game.reset()\n",
        "      agent.n_games += 1\n",
        "      agent.train_long_memory()\n",
        "\n",
        "      if score > record:\n",
        "        record = score\n",
        "        agent.model.save()\n",
        "\n",
        "      print('Game', agent.ngames, 'Score', score, 'Record:', record)\n",
        "\n",
        "      plot_scores.append(score)\n",
        "      total_score += score\n",
        "      mean_score = total_score / agent.n_games\n",
        "      plot_mean_scores.append(mean_score)\n",
        "\n",
        "      plot(plot_scores, plot_mean_scores)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfRpH01SVrzE",
        "outputId": "7e68e9e8-fe27-49b1-9f6a-ee766af35bc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile game.py\n",
        "\n",
        "import pygame\n",
        "import random\n",
        "from enum import Enum\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "\n",
        "pygame.init()\n",
        "font = pygame.font.SysFont('arial', 25)\n",
        "\n",
        "class Direction(Enum):\n",
        "  RIGHT, LEFT, UP, DOWN = 1, 2, 3, 4\n",
        "\n",
        "Point = namedtuple('Point', 'x, y')\n",
        "\n",
        "WHITE = (255, 255, 255)\n",
        "RED = (200, 0, 0)\n",
        "BLUE1 = (0, 0, 255)\n",
        "BLUE2 = (0, 100, 255)\n",
        "BLACK = (0, 0, 0)\n",
        "\n",
        "BLOCK_SIZE = 20\n",
        "SPEED = 40\n",
        "\n",
        "class SnakeGameAI():\n",
        "  def __init__(self, w = 640, h = 480):\n",
        "    self.w = w\n",
        "    self.h = h\n",
        "\n",
        "    self.display = pygame.display.set_mode((self.w, self.h))\n",
        "    pygame.display.set_caption('Snake')\n",
        "\n",
        "    #Initialize the time\n",
        "    self.clock = pygame.time.Clock()\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "\n",
        "    #Initialize the direction moving\n",
        "    self.direction = Direction.RIGHT\n",
        "\n",
        "    #Initialize a head at the center\n",
        "    self.head = Point(self.w/2, self.h/2)\n",
        "\n",
        "    #Initialize the snake with 3 blocks\n",
        "    self.snake = [self.head, Point(self.head.x - BLOCK_SIZE, self.head.y),\n",
        "                  Point(self.head.x - 2 * BLOCK_SIZE, self.head.y)]\n",
        "\n",
        "    self.score = 0\n",
        "    self.food = None\n",
        "    self._place_food()\n",
        "\n",
        "    self.frame_iteration = 0\n",
        "\n",
        "  #randomly place a food at a point, if inside the snake, free point (grid)\n",
        "  def _place_food(self):\n",
        "    x = random.randint(0, (self.w - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    y = random.randint(0, (self.h - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    self.food = Point(x, y)\n",
        "\n",
        "    if self.food in self.snake:\n",
        "      self._place_food()\n",
        "\n",
        "\n",
        "  def play_step(self, action):\n",
        "\n",
        "    self.frame_iteration += 1\n",
        "    for event in pygame.event.get():\n",
        "\n",
        "      if event.type == pygame.QUIT:\n",
        "        pygame.quit()\n",
        "        quit()\n",
        "\n",
        "    #Move the head\n",
        "    self._move(action)\n",
        "\n",
        "    #Grow the snake\n",
        "    self.snake.insert(0, self.head)\n",
        "\n",
        "    #Check if game is over\n",
        "    reward = 0\n",
        "    game_over = False\n",
        "    if self._is_collision() or self.frame_iteration > 100 * len(self.snake):\n",
        "      game_over = True\n",
        "      reward = -10\n",
        "      return game_over, self.score\n",
        "\n",
        "    #Place new food or dont grow\n",
        "\n",
        "    if self.head == self.food:\n",
        "      self.score += 1\n",
        "      reward = 10\n",
        "      self._place_food()\n",
        "    else:\n",
        "      self.snake.pop()\n",
        "\n",
        "    #update the ui and clock\n",
        "    self._update_ui()\n",
        "\n",
        "    #the timestep\n",
        "    self.clock.tick(SPEED)\n",
        "\n",
        "    return reward, game_over, self.score\n",
        "\n",
        "  def _is_collision(self, pt=None):\n",
        "\n",
        "    if pt is None:\n",
        "      pt = self.head\n",
        "\n",
        "    if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
        "      return True\n",
        "\n",
        "    if self.head in self.snake[1:]:\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "  def _update_ui(self):\n",
        "\n",
        "    self.display.fill(BLACK)\n",
        "\n",
        "    #Draw the blue snake\n",
        "    for pt in self.snake:\n",
        "      pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "      pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x + 4, pt.y + 4, 12, 12))\n",
        "\n",
        "    #Draw the red food\n",
        "    pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
        "\n",
        "    #Update the font\n",
        "    text = font.render(\"Score: \" + str(self.score), True, WHITE)\n",
        "    self.display.blit(text, [0, 0])\n",
        "    pygame.display.flip()\n",
        "\n",
        "  def _move(self, action):\n",
        "\n",
        "    clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
        "    idx = clock_wise.index(self.direction)\n",
        "\n",
        "    if np.array_equal(action, [1, 0, 0]):\n",
        "      new_dir = clock_wise[idx]\n",
        "    elif np.array_equal(action, [0, 1, 0]):\n",
        "      next_idx = (idx + 1) % 4\n",
        "      new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
        "    else: # [0, 0, 1]\n",
        "      next_idx = (idx - 1) % 4\n",
        "      new_dir = clock_wise[next_idx]\n",
        "\n",
        "    self.direction = new_dir\n",
        "\n",
        "    x = self.head.x\n",
        "    y = self.head.y\n",
        "\n",
        "    if self.direction == Direction.RIGHT:\n",
        "        x += BLOCK_SIZE\n",
        "    elif self.direction == Direction.LEFT:\n",
        "        x -= BLOCK_SIZE\n",
        "    elif self.direction == Direction.DOWN:\n",
        "        y += BLOCK_SIZE\n",
        "    elif self.direction == Direction.UP:\n",
        "        y -= BLOCK_SIZE\n",
        "\n",
        "    self.head = Point(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q85cI0a4WBrF",
        "outputId": "93e1801a-c4e7-4c48-85a6-02789c4dda62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing game.py\n"
          ]
        }
      ]
    }
  ]
}